{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOnSh375CRZKYHZyJ6ct+7N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install Required Libraries"],"metadata":{"id":"8u3idwfmeBRr"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"dwsDw_vFx3St","executionInfo":{"status":"ok","timestamp":1760299855215,"user_tz":240,"elapsed":6500,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}}},"outputs":[],"source":["# Install kagglehub for downloading the dataset\n","!pip install -q kagglehub"]},{"cell_type":"markdown","source":["kagglehub allows us to download datasets from Kaggle easily."],"metadata":{"id":"AwsaEpCKdnfd"}},{"cell_type":"code","source":["# Install OpenCV (cv2)\n","!pip install -q opencv-python"],"metadata":{"id":"JNmrEEFZ5BP7","executionInfo":{"status":"ok","timestamp":1760299859498,"user_tz":240,"elapsed":4280,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Import All Required Libraries"],"metadata":{"id":"BleR3Ztud9VE"}},{"cell_type":"code","source":["import os                      # For file and directory operations\n","import numpy as np             # For numerical operations\n","import cv2                     # For image processing (reading, resizing)\n","import torch                   # PyTorch for creating tensors\n","import kagglehub              # For downloading dataset from Kaggle\n","from pathlib import Path       # For handling file paths elegantly\n","import glob                    # For finding files matching patterns\n","from tqdm import tqdm          # For progress bars\n","import random                  # For random operations\n","# from google.colab import drive # For mounting Google Drive\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3xdgPQXfdfMn","executionInfo":{"status":"ok","timestamp":1760299863924,"user_tz":240,"elapsed":4424,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}},"outputId":"1b0e874b-73ce-40be-cc84-da43454b42e1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.8.0+cu126\n","CUDA available: True\n"]}]},{"cell_type":"markdown","source":["#  Set Random Seeds for Reproducibility"],"metadata":{"id":"KKuBJVzCeFKA"}},{"cell_type":"code","source":["# Set random seeds so results are reproducible\n","RANDOM_SEED = 42\n","\n","random.seed(RANDOM_SEED)           # Python's random module\n","np.random.seed(RANDOM_SEED)        # NumPy's random\n","torch.manual_seed(RANDOM_SEED)     # PyTorch CPU operations\n","torch.cuda.manual_seed(RANDOM_SEED) # PyTorch GPU operations\n","\n","print(f\"Random seed set to {RANDOM_SEED}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hs_KhMZdx99","executionInfo":{"status":"ok","timestamp":1760299863961,"user_tz":240,"elapsed":24,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}},"outputId":"1208e212-5328-4051-f027-9bf6e1413e2f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Random seed set to 42\n"]}]},{"cell_type":"markdown","source":["RANDOM_SEED = 42 - Sets our magic number (42 is a convention, any number works) <br>\n","random.seed(RANDOM_SEED) - Sets seed for Python's built-in random module <br>\n","np.random.seed(RANDOM_SEED) - Sets seed for NumPy's random operations <br>\n","torch.manual_seed(RANDOM_SEED) - Sets seed for PyTorch operations on CPU <br>\n","torch.cuda.manual_seed(RANDOM_SEED) - Sets seed for PyTorch operations on GPU <br>\n","\n"],"metadata":{"id":"xn-iYRwIeSxv"}},{"cell_type":"markdown","source":["# Configure Kaggle Credentials"],"metadata":{"id":"Jjfaa6nyeeVJ"}},{"cell_type":"code","source":["# Set up Kaggle API credentials as environment variables\n","os.environ['KAGGLE_USERNAME'] = \"mamaniaharsh7\"\n","os.environ['KAGGLE_KEY'] = \"874d22c6d4c17ee2a0b9095c121f2cdc\""],"metadata":{"id":"TrFYYASCeOHb","executionInfo":{"status":"ok","timestamp":1760299863964,"user_tz":240,"elapsed":2,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"fgCqOpVLencX"}},{"cell_type":"markdown","source":["# Define Configuration Parameters"],"metadata":{"id":"h56XrjzReoCb"}},{"cell_type":"code","source":["# Configuration for our dataset creation\n","CONFIG = {\n","    'target_frames': 100000,      # 100k frames for full dataset\n","    'image_size': 64,             # Resize all images to 64x64\n","    'normalization': [0, 1],      # Normalize pixel values to [0, 1] range\n","    'dtype': torch.float32,       # Use float32 precision\n","    'label_real': 0,              # Label for real videos\n","    'label_fake': 1,              # Label for fake videos\n","}"],"metadata":{"id":"2Q-dLtXleqOV","executionInfo":{"status":"ok","timestamp":1760299863987,"user_tz":240,"elapsed":3,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Creates a dictionary called CONFIG that stores all our settings in one place <br>\n","'target_frames': 1000 - We'll start with 1,000 frames as a test <br>\n","'image_size': 64 - All images will be resized to 64√ó64 pixels <br>\n","'normalization': [0, 1] - Pixel values will be scaled from [0, 255] to [0, 1] <br>\n","'dtype': torch.float32 - Tensors will use 32-bit floating point precision <br>\n","'label_real': 0 - Real videos get label 0 <br>\n","'label_fake': 1 - Fake videos get label 1 <br>"],"metadata":{"id":"TS_kcqyje0l5"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"rx6JdIVp2pQg"}},{"cell_type":"markdown","source":["# Optional cells"],"metadata":{"id":"GbL0MHk02qRU"}},{"cell_type":"markdown","source":["## Memory Cleanup Cell"],"metadata":{"id":"ugh6mbcH2cn8"}},{"cell_type":"code","source":["# Delete all variables from the 1k test to free memory\n","variables_to_delete = [\n","    'original_frames', 'deepfake_frames',\n","    'original_frames_resized', 'deepfake_frames_resized',\n","    'original_frames_normalized', 'deepfake_frames_normalized',\n","    'original_tensors', 'deepfake_tensors',\n","    'X_original', 'X_deepfake', 'X', 'y',\n","    'mean', 'std', 'dataset_stats',\n","    'original_frames_frames', 'deepfake_frames_frames'\n","]\n","\n","deleted_count = 0\n","for var_name in variables_to_delete:\n","    if var_name in globals():\n","        del globals()[var_name]\n","        deleted_count += 1\n","        print(f\"   ‚úì Deleted: {var_name}\")\n","\n","print(f\"\\n‚úÖ Cleared {deleted_count} variables from memory\")\n","\n","# Run garbage collection to free memory\n","import gc\n","gc.collect()\n","\n","print(f\"‚úÖ Garbage collection complete\")\n","print(f\"\\nüíæ Memory freed and ready for 100k processing!\")\n","\n","print(f\"\\n‚è±Ô∏è Estimated time for 100k:\")\n","print(f\"   ‚Ä¢ Frame extraction: ~40-50 minutes\")\n","print(f\"   ‚Ä¢ Processing: ~15 minutes\")\n","print(f\"   ‚Ä¢ Saving & compression: ~5 minutes\")\n","print(f\"   ‚Ä¢ Total: ~60-75 minutes\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d47MUVmi2eBN","executionInfo":{"status":"ok","timestamp":1760299864145,"user_tz":240,"elapsed":101,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}},"outputId":"e1df2e4e-dd7e-4f57-a9da-224b5f097a64"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Cleared 0 variables from memory\n","‚úÖ Garbage collection complete\n","\n","üíæ Memory freed and ready for 100k processing!\n","\n","‚è±Ô∏è Estimated time for 100k:\n","   ‚Ä¢ Frame extraction: ~40-50 minutes\n","   ‚Ä¢ Processing: ~15 minutes\n","   ‚Ä¢ Saving & compression: ~5 minutes\n","   ‚Ä¢ Total: ~60-75 minutes\n"]}]},{"cell_type":"markdown","source":["## Monitoring Cell"],"metadata":{"id":"Bp5Ze60Y2tBJ"}},{"cell_type":"code","source":["# # ========================================\n","# # OPTIONAL: REAL-TIME MONITORING\n","# # Run this cell to monitor progress\n","# # (Run in background while other cells execute)\n","# # ========================================\n","\n","# import time\n","# import psutil\n","# from IPython.display import clear_output\n","\n","# print(\"üìä REAL-TIME RESOURCE MONITORING\")\n","# print(\"=\"*60)\n","# print(\"This cell will update every 10 seconds\")\n","# print(\"Press the STOP button (‚ñ†) to stop monitoring\\n\")\n","\n","# try:\n","#     iteration = 0\n","#     while True:\n","#         iteration += 1\n","\n","#         # Memory usage\n","#         memory = psutil.virtual_memory()\n","#         memory_used_gb = memory.used / (1024**3)\n","#         memory_total_gb = memory.total / (1024**3)\n","#         memory_percent = memory.percent\n","\n","#         # Disk usage\n","#         disk = psutil.disk_usage('/content')\n","#         disk_used_gb = disk.used / (1024**3)\n","#         disk_total_gb = disk.total / (1024**3)\n","#         disk_percent = disk.percent\n","\n","#         # Clear previous output and show new stats\n","#         clear_output(wait=True)\n","\n","#         print(\"üìä REAL-TIME RESOURCE MONITORING\")\n","#         print(\"=\"*60)\n","#         print(f\"Update #{iteration} - {time.strftime('%H:%M:%S')}\")\n","#         print(f\"\\nüíæ RAM Usage:\")\n","#         print(f\"   {memory_used_gb:.2f} GB / {memory_total_gb:.2f} GB ({memory_percent:.1f}%)\")\n","#         print(f\"   {'‚ñà' * int(memory_percent / 2)}\")\n","\n","#         print(f\"\\nüíø Disk Usage:\")\n","#         print(f\"   {disk_used_gb:.1f} GB / {disk_total_gb:.1f} GB ({disk_percent:.1f}%)\")\n","#         print(f\"   {'‚ñà' * int(disk_percent / 2)}\")\n","\n","#         print(f\"\\nüí° Tips:\")\n","#         if memory_percent > 80:\n","#             print(f\"   ‚ö†Ô∏è High memory usage - this is normal for 100k frames\")\n","#         if disk_percent > 80:\n","#             print(f\"   ‚ö†Ô∏è High disk usage - will clean up after compression\")\n","\n","#         print(f\"\\n‚èπÔ∏è Press STOP button (‚ñ†) to stop monitoring\")\n","\n","#         time.sleep(10)  # Update every 10 seconds\n","\n","# except KeyboardInterrupt:\n","#     print(\"\\n\\n‚úÖ Monitoring stopped by user\")"],"metadata":{"id":"kcX8_-zN25iC","executionInfo":{"status":"ok","timestamp":1760299864228,"user_tz":240,"elapsed":80,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"wVHOJrk9fB3Q"}},{"cell_type":"markdown","source":["# SECTION 2: Download FaceForensics++ Dataset"],"metadata":{"id":"CWq9PimufRue"}},{"cell_type":"markdown","source":["## Cell 2.1: Download Dataset from Kaggle"],"metadata":{"id":"WFoX1j0GfTv8"}},{"cell_type":"code","source":["import time\n","\n","# Start timer to measure download time\n","start_time = time.time()\n","\n","# Download the dataset using kagglehub\n","path = kagglehub.dataset_download(\"xdxd003/ff-c23\")\n","\n","# Calculate elapsed time\n","elapsed_time = time.time() - start_time\n","\n","print(f\"\\n‚úÖ Download completed in {elapsed_time/60:.2f} minutes\")\n","print(f\"üìÅ Dataset location: {path}\")\n","\n","# Store the path for later use\n","DATASET_PATH = path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3hsVcxqexfO","executionInfo":{"status":"ok","timestamp":1760300479530,"user_tz":240,"elapsed":615300,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}},"outputId":"e11ea76c-4695-4d16-b119-40397e297111"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/xdxd003/ff-c23?dataset_version_number=1...\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.7G/16.7G [09:16<00:00, 32.1MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Download completed in 10.26 minutes\n","üìÅ Dataset location: /root/.cache/kagglehub/datasets/xdxd003/ff-c23/versions/1\n"]}]},{"cell_type":"markdown","source":["import time - Import time module to measure download duration <br>\n","start_time = time.time() - Record the current timestamp before download starts <br><br>\n","path = kagglehub.dataset_download(\"xdxd003/ff-c23\") - Main download command <br>\n","Downloads the FaceForensics++ dataset (c23 compression) <br>\n","Returns the path where dataset is stored <br>\n","Kagglehub caches it so re-running won't re-download <br>\n","<br>\n","elapsed_time = time.time() - start_time - Calculate how long download took <br>\n","DATASET_PATH = path - Save the path in a variable we can use throughout the notebook <br>"],"metadata":{"id":"NG2qQLkYfsTW"}},{"cell_type":"markdown","source":["## Cell 2.2: Explore Dataset Structure"],"metadata":{"id":"n-eu410If9oZ"}},{"cell_type":"code","source":["# List all items in the dataset directory\n","print(f\"\\nüìÅ Contents of: {DATASET_PATH}\")\n","items = sorted(os.listdir(DATASET_PATH))\n","\n","for item in items:\n","    item_path = os.path.join(DATASET_PATH, item)\n","    if os.path.isdir(item_path):\n","        # Count files in this directory\n","        num_files = len(os.listdir(item_path))\n","        print(f\"  üìÅ {item}/ ({num_files} files)\")\n","    else:\n","        # Show file size\n","        size_mb = os.path.getsize(item_path) / (1024**2)\n","        print(f\"  üìÑ {item} ({size_mb:.2f} MB)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EnyVTY1lfjwW","executionInfo":{"status":"ok","timestamp":1760300479639,"user_tz":240,"elapsed":79,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}},"outputId":"2f30fcea-2f74-4f1f-93dd-ebea6a970a00"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìÅ Contents of: /root/.cache/kagglehub/datasets/xdxd003/ff-c23/versions/1\n","  üìÅ FaceForensics++_C23/ (8 files)\n"]}]},{"cell_type":"markdown","source":["os.listdir(DATASET_PATH) - Gets list of all files/folders in the dataset directory <br>\n","sorted(...) - Sorts them alphabetically for easier reading <br>\n","for item in items: - Loop through each item <br>\n","item_path = os.path.join(DATASET_PATH, item) - Creates full path to the item <br>\n","os.path.isdir(item_path) - Checks if item is a directory (folder) <br>\n","len(os.listdir(item_path)) - Counts how many files are inside the folder <br>\n","os.path.getsize(item_path) - Gets file size in bytes <br>\n","/ (1024**2) - Converts bytes to megabytes (MB) <br>\n","<br>\n","Expected output: You'll see folders like:\n","\n","original/ (1000 files) <br>\n","Deepfakes/ (1000 files) <br>\n","csv/ (10 files)<br>\n","And other manipulation folders <br>"],"metadata":{"id":"GXLdR27BgF0s"}},{"cell_type":"markdown","source":["## Cell 2.3: Locate Original and Deepfake Video Folders\n"],"metadata":{"id":"gnNHukLqglMe"}},{"cell_type":"code","source":["# The dataset has an extra subfolder level\n","# Update the base path to include the FaceForensics++_C23 folder\n","DATASET_PATH = os.path.join(DATASET_PATH, \"FaceForensics++_C23\")\n","\n","print(f\"üìÅ Updated dataset path: {DATASET_PATH}\")\n","\n","# Now let's see what's inside\n","print(f\"\\nüìÇ Contents of FaceForensics++_C23:\")\n","items = sorted(os.listdir(DATASET_PATH))\n","for item in items:\n","    item_path = os.path.join(DATASET_PATH, item)\n","    if os.path.isdir(item_path):\n","        num_files = len(os.listdir(item_path))\n","        print(f\"  üìÅ {item}/ ({num_files} files)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlTYwnfmgk5w","executionInfo":{"status":"ok","timestamp":1760300479674,"user_tz":240,"elapsed":32,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}},"outputId":"71a49e49-b681-4c21-8e5c-a07d0fc0ae8a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["üìÅ Updated dataset path: /root/.cache/kagglehub/datasets/xdxd003/ff-c23/versions/1/FaceForensics++_C23\n","\n","üìÇ Contents of FaceForensics++_C23:\n","  üìÅ DeepFakeDetection/ (1000 files)\n","  üìÅ Deepfakes/ (1000 files)\n","  üìÅ Face2Face/ (1000 files)\n","  üìÅ FaceShifter/ (1000 files)\n","  üìÅ FaceSwap/ (1000 files)\n","  üìÅ NeuralTextures/ (1000 files)\n","  üìÅ csv/ (10 files)\n","  üìÅ original/ (1000 files)\n"]}]},{"cell_type":"code","source":["# Define paths to the two folders we need\n","original_folder = os.path.join(DATASET_PATH, \"original\")\n","deepfake_folder = os.path.join(DATASET_PATH, \"Deepfakes\")\n","\n","# Get list of all video files in each folder\n","original_videos = sorted(glob.glob(os.path.join(original_folder, \"*.mp4\")))\n","deepfake_videos = sorted(glob.glob(os.path.join(deepfake_folder, \"*.mp4\")))\n","\n","print(f\"\\nüìä Found {len(original_videos)} original videos\")\n","print(f\"üìä Found {len(deepfake_videos)} deepfake videos\")\n","print(f\"üìä Total videos: {len(original_videos) + len(deepfake_videos)}\")\n","\n","# Display sample video names\n","if len(original_videos) > 0:\n","    print(f\"\\nüìù Sample original videos:\")\n","    for i, video in enumerate(original_videos[:3], 1):\n","        print(f\"  {i}. {os.path.basename(video)}\")\n","\n","if len(deepfake_videos) > 0:\n","    print(f\"\\nüìù Sample deepfake videos:\")\n","    for i, video in enumerate(deepfake_videos[:3], 1):\n","        print(f\"  {i}. {os.path.basename(video)}\")\n","\n","# Verify we have the expected number\n","if len(original_videos) == 1000 and len(deepfake_videos) == 1000:\n","    print(f\"\\n‚úÖ Perfect! Found all 2000 videos (1000 real + 1000 fake)\")\n","else:\n","    print(f\"\\n‚ö†Ô∏è Warning: Expected 2000 videos total, found {len(original_videos) + len(deepfake_videos)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucpnRqHxhXaD","executionInfo":{"status":"ok","timestamp":1760300479701,"user_tz":240,"elapsed":22,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}},"outputId":"cccc3cbc-f0a4-4062-d6b8-6567bacefcef"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìä Found 1000 original videos\n","üìä Found 1000 deepfake videos\n","üìä Total videos: 2000\n","\n","üìù Sample original videos:\n","  1. 000.mp4\n","  2. 001.mp4\n","  3. 002.mp4\n","\n","üìù Sample deepfake videos:\n","  1. 000_003.mp4\n","  2. 001_870.mp4\n","  3. 002_006.mp4\n","\n","‚úÖ Perfect! Found all 2000 videos (1000 real + 1000 fake)\n"]}]},{"cell_type":"markdown","source":["os.path.join(DATASET_PATH, \"original\") - Creates the full path to the original videos folder <br>  <br>\n","glob.glob(os.path.join(original_folder, \"*.mp4\")) - Finds all files matching the pattern  <br>\n","\n","\"*.mp4\" means \"any file ending with .mp4\"  <br>\n","Returns a list of full paths to all MP4 files  <br>\n"," <br>\n","\n","sorted(...) - Sorts the video list alphabetically (important for reproducibility)  <br>\n","len(original_videos) - Counts how many videos we found  <br>\n","enumerate(original_videos[:3], 1) - Takes first 3 videos and numbers them starting from 1  <br>\n","os.path.basename(video) - Extracts just the filename (without the full path)  <br>\n","The verification at the end confirms we have exactly 1000 of each type  <br>"],"metadata":{"id":"DVlEFhTogrEG"}},{"cell_type":"markdown","source":["## Cell 2.4: Calculate Frames Needed Per Video\n"],"metadata":{"id":"Yq5CSAtRhgm9"}},{"cell_type":"code","source":["# # Total frames we want\n","# target_total_frames = CONFIG['target_frames']\n","\n","# # Split equally between real and fake\n","# frames_per_category = target_total_frames // 2\n","\n","# # Calculate frames needed per video\n","# num_original_videos = len(original_videos)\n","# num_deepfake_videos = len(deepfake_videos)\n","\n","# frames_per_original_video = frames_per_category // num_original_videos\n","# frames_per_deepfake_video = frames_per_category // num_deepfake_videos\n","\n","# print(f\"üéØ Target: {target_total_frames:,} total frames\")\n","# print(f\"   ‚Ä¢ {frames_per_category:,} from original videos\")\n","# print(f\"   ‚Ä¢ {frames_per_category:,} from deepfake videos\")\n","# print(f\"\\nüìä Frame extraction plan:\")\n","# print(f\"   ‚Ä¢ {frames_per_original_video} frames per original video (√ó {num_original_videos} videos)\")\n","# print(f\"   ‚Ä¢ {frames_per_deepfake_video} frames per deepfake video (√ó {num_deepfake_videos} videos)\")\n","\n","# # Calculate actual totals (accounting for integer division)\n","# actual_original_frames = frames_per_original_video * num_original_videos\n","# actual_deepfake_frames = frames_per_deepfake_video * num_deepfake_videos\n","# actual_total = actual_original_frames + actual_deepfake_frames\n","\n","# print(f\"\\n‚úÖ Actual frames we'll extract:\")\n","# print(f\"   ‚Ä¢ Original: {actual_original_frames:,} frames\")\n","# print(f\"   ‚Ä¢ Deepfake: {actual_deepfake_frames:,} frames\")\n","# print(f\"   ‚Ä¢ Total: {actual_total:,} frames\")\n","\n","# if actual_total != target_total_frames:\n","#     print(f\"\\n‚ö†Ô∏è Note: {target_total_frames - actual_total} frames difference due to integer division\")"],"metadata":{"id":"gwXX5LQ5g-7W","executionInfo":{"status":"ok","timestamp":1760300479707,"user_tz":240,"elapsed":3,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["target_total_frames = CONFIG['target_frames'] - Gets our target (1000 for test, 100k for full run) <br><br>\n","frames_per_category = target_total_frames // 2 - Splits equally (500 real + 500 fake for test)<br>\n","\n","// is integer division (no decimals)\n","<br><br>\n","\n","frames_per_original_video = frames_per_category // num_original_videos - Calculates frames per video<br>\n","\n","For test: 500 √∑ 1000 = 0 frames per video (this is a problem for the test!)<br>\n","For full: 50,000 √∑ 1000 = 50 frames per video ‚úì<br>\n","\n","<br><br>\n","actual_original_frames = frames_per_original_video * num_original_videos - Calculates actual total<br>"],"metadata":{"id":"ETHPMtwUhnfx"}},{"cell_type":"markdown","source":["- Adjust Strategy for Small Test (IMPORTANT FIX)"],"metadata":{"id":"j_0PFQKgiaB9"}},{"cell_type":"code","source":["print(\"\\nüìê Calculating frame distribution...\")\n","print(\"-\"*60)\n","\n","# Calculate target frames and split\n","target_total_frames = CONFIG['target_frames']\n","frames_per_category = target_total_frames // 2\n","\n","print(f\"üéØ Target: {target_total_frames:,} total frames\")\n","print(f\"   ‚Ä¢ {frames_per_category:,} from original videos\")\n","print(f\"   ‚Ä¢ {frames_per_category:,} from deepfake videos\")\n","\n","# For small tests, we need to select a subset of videos\n","# For full run, we use all videos\n","\n","if target_total_frames < len(original_videos) + len(deepfake_videos):\n","    # SMALL TEST MODE: Select subset of videos\n","    print(\"\\nüìå Test mode: Selecting subset of videos\")\n","\n","    # Calculate how many videos we need to reach our target\n","    videos_per_category = max(1, target_total_frames // 100)  # At least 50 frames per video\n","\n","    # Randomly select videos (using our seed for reproducibility)\n","    selected_original_videos = random.sample(original_videos, videos_per_category)\n","    selected_deepfake_videos = random.sample(deepfake_videos, videos_per_category)\n","\n","    # Recalculate frames per video\n","    frames_per_original_video = frames_per_category // len(selected_original_videos)\n","    frames_per_deepfake_video = frames_per_category // len(selected_deepfake_videos)\n","\n","    print(f\"   ‚úì Selected {len(selected_original_videos)} original videos\")\n","    print(f\"   ‚úì Selected {len(selected_deepfake_videos)} deepfake videos\")\n","    print(f\"   ‚úì Will extract ~{frames_per_original_video} frames per video\")\n","\n","else:\n","    # FULL MODE: Use all videos\n","    print(\"\\nüìå Full mode: Using all videos\")\n","    selected_original_videos = original_videos\n","    selected_deepfake_videos = deepfake_videos\n","\n","    frames_per_original_video = frames_per_category // len(original_videos)\n","    frames_per_deepfake_video = frames_per_category // len(deepfake_videos)\n","\n","    print(f\"   ‚úì Using all {len(original_videos)} original videos\")\n","    print(f\"   ‚úì Using all {len(deepfake_videos)} deepfake videos\")\n","    print(f\"   ‚úì Will extract {frames_per_original_video} frames per video\")\n","\n","print(f\"\\n‚úÖ Final extraction plan:\")\n","print(f\"   Videos: {len(selected_original_videos)} original + {len(selected_deepfake_videos)} deepfake\")\n","print(f\"   Frames per video: ~{frames_per_original_video}\")\n","print(f\"   Target total: ~{target_total_frames:,} frames\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MrXFAIwDibBC","executionInfo":{"status":"ok","timestamp":1760300479730,"user_tz":240,"elapsed":19,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}},"outputId":"9ba41c7d-914d-4d48-c57d-8d2440440834"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìê Calculating frame distribution...\n","------------------------------------------------------------\n","üéØ Target: 100,000 total frames\n","   ‚Ä¢ 50,000 from original videos\n","   ‚Ä¢ 50,000 from deepfake videos\n","\n","üìå Full mode: Using all videos\n","   ‚úì Using all 1000 original videos\n","   ‚úì Using all 1000 deepfake videos\n","   ‚úì Will extract 50 frames per video\n","\n","‚úÖ Final extraction plan:\n","   Videos: 1000 original + 1000 deepfake\n","   Frames per video: ~50\n","   Target total: ~100,000 frames\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"6vkYPHnEi00M"}},{"cell_type":"markdown","source":["# SECTION 3: Frame Extraction"],"metadata":{"id":"ZXQEo3qBi7C8"}},{"cell_type":"markdown","source":["## Cell 3.1: Define Frame Extraction Function"],"metadata":{"id":"M7xgGMiWi9ph"}},{"cell_type":"code","source":["def extract_frames_evenly(video_path, num_frames):\n","    \"\"\"\n","    Extract evenly spaced frames from a video.\n","\n","    Args:\n","        video_path (str): Path to the video file\n","        num_frames (int): Number of frames to extract\n","\n","    Returns:\n","        list: List of numpy arrays (frames in BGR format)\n","    \"\"\"\n","    frames = []  # Store extracted frames here\n","\n","    try:\n","        # Open the video file\n","        cap = cv2.VideoCapture(video_path)\n","\n","        # Get total number of frames in the video\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","        # Check if video opened successfully\n","        if total_frames == 0:\n","            print(f\"‚ö†Ô∏è Warning: Could not read {os.path.basename(video_path)}\")\n","            cap.release()\n","            return frames\n","\n","        # Calculate which frame indices to extract (evenly spaced)\n","        # np.linspace creates evenly spaced numbers\n","        # Example: if total_frames=300 and num_frames=10\n","        # This gives: [0, 33, 66, 99, 133, 166, 199, 233, 266, 299]\n","        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n","\n","        # Extract each frame\n","        for frame_idx in frame_indices:\n","            # Jump to the specific frame\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n","\n","            # Read the frame\n","            ret, frame = cap.read()\n","\n","            # Check if frame was read successfully\n","            if ret:\n","                # Convert from BGR (OpenCV format) to RGB (standard format)\n","                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                frames.append(frame_rgb)\n","            else:\n","                print(f\"‚ö†Ô∏è Failed to read frame {frame_idx} from {os.path.basename(video_path)}\")\n","\n","        # Release the video file\n","        cap.release()\n","\n","    except Exception as e:\n","        # Catch any errors during processing\n","        print(f\"‚ùå Error processing {os.path.basename(video_path)}: {str(e)}\")\n","\n","    return frames"],"metadata":{"id":"QrJHekACifTZ","executionInfo":{"status":"ok","timestamp":1760300479737,"user_tz":240,"elapsed":3,"user":{"displayName":"Harsh Mamania","userId":"16086672063323716803"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["def extract_frames_evenly(video_path, num_frames): - Defines a function that takes video path and desired frame count\n","frames = [] - Creates empty list to store extracted frames\n","\n","Open video:\n","\n","cap = cv2.VideoCapture(video_path) - Opens the video file using OpenCV\n","total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - Gets total number of frames in the video\n","if total_frames == 0: - Checks if video is corrupted or unreadable\n","\n","Calculate frame positions:\n","\n","frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int) - Key line!\n","\n","Creates evenly spaced frame indices\n","Example: For 300-frame video wanting 10 frames ‚Üí [0, 33, 66, 99, 133, 166, 199, 233, 266, 299]\n","dtype=int ensures we get whole numbers (can't extract frame 33.5)\n","\n","\n","\n","Extract frames:\n","\n","for frame_idx in frame_indices: - Loop through each frame index we calculated\n","cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx) - Jumps video to specific frame number\n","ret, frame = cap.read() - Reads that frame\n","\n","ret is True/False (success/failure)\n","frame is the actual image as numpy array\n","\n","\n","cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) - Converts color format\n","\n","OpenCV reads images in BGR (Blue-Green-Red)\n","We need RGB (Red-Green-Blue) - standard format\n","\n","\n","frames.append(frame_rgb) - Adds frame to our list\n","\n","Cleanup:\n","\n","cap.release() - Closes the video file (important to free memory!)\n","except Exception as e: - Catches any errors so one bad video doesn't crash everything"],"metadata":{"id":"6j2nnlCJjGZ6"}},{"cell_type":"markdown","source":["## Cell 3.2: Extract Frames from Original Videos\n"],"metadata":{"id":"bKcyiyZKjw18"}},{"cell_type":"code","source":["# Storage for all frames\n","original_frames = []\n","\n","# Use tqdm for progress bar\n","for video_path in tqdm(selected_original_videos, desc=\"Processing original videos\"):\n","    # Extract frames from this video\n","    frames = extract_frames_evenly(video_path, frames_per_original_video)\n","\n","    # Add all extracted frames to our collection\n","    original_frames.extend(frames)\n","\n","print(f\"\\n‚úÖ Extracted {len(original_frames)} frames from {len(selected_original_videos)} original videos\")\n","print(f\"   Average: {len(original_frames) / len(selected_original_videos):.1f} frames per video\")\n","\n","# Check first frame properties\n","if len(original_frames) > 0:\n","    sample_frame = original_frames[0]\n","    print(f\"\\nüñºÔ∏è Sample frame properties:\")\n","    print(f\"   Shape: {sample_frame.shape} (Height √ó Width √ó Channels)\")\n","    print(f\"   Data type: {sample_frame.dtype}\")\n","    print(f\"   Value range: [{sample_frame.min()}, {sample_frame.max()}]\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQXBWsSOjzBx","outputId":"6db4b4fc-03ad-43e6-9f46-fe953730ea60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing original videos:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 504/1000 [44:12<44:20,  5.36s/it]"]}]},{"cell_type":"markdown","source":["original_frames = [] - Creates empty list to store ALL frames from ALL original videos\n","for video_path in tqdm(selected_original_videos, desc=\"...\"): - Loop through each video\n","\n","tqdm() wraps the loop to show a progress bar\n","desc=\"...\" sets the progress bar description\n","\n","\n","frames = extract_frames_evenly(video_path, frames_per_original_video) - Extracts frames from one video\n","\n","Uses our function from Cell 3.1\n","frames_per_original_video was calculated in Section 2 (~50 for test, 50 for full)\n","\n","\n","original_frames.extend(frames) - Adds frames to our collection\n","\n",".extend() adds all items from frames list to original_frames list\n","Different from .append() which would add the entire list as one item\n","\n","\n","len(original_frames) - Counts total frames extracted\n","sample_frame.shape - Shows dimensions (e.g., (720, 1280, 3) = 720 height √ó 1280 width √ó 3 color channels)\n","sample_frame.dtype - Shows data type (e.g., uint8 = unsigned 8-bit integer, values 0-255)\n","sample_frame.min() and .max() - Shows value range (should be 0-255 for raw images)"],"metadata":{"id":"ppEo5gajj4L3"}},{"cell_type":"markdown","source":["## Cell 3.3: Extract Frames from Deepfake Videos\n"],"metadata":{"id":"Nctu5_aDkTlf"}},{"cell_type":"code","source":["# Storage for all frames\n","deepfake_frames = []\n","\n","# Use tqdm for progress bar\n","for video_path in tqdm(selected_deepfake_videos, desc=\"Processing deepfake videos\"):\n","    # Extract frames from this video\n","    frames = extract_frames_evenly(video_path, frames_per_deepfake_video)\n","\n","    # Add all extracted frames to our collection\n","    deepfake_frames.extend(frames)\n","\n","print(f\"\\n‚úÖ Extracted {len(deepfake_frames)} frames from {len(selected_deepfake_videos)} deepfake videos\")\n","print(f\"   Average: {len(deepfake_frames) / len(selected_deepfake_videos):.1f} frames per video\")\n","\n","# Check first frame properties\n","if len(deepfake_frames) > 0:\n","    sample_frame = deepfake_frames[0]\n","    print(f\"\\nüñºÔ∏è Sample frame properties:\")\n","    print(f\"   Shape: {sample_frame.shape} (Height √ó Width √ó Channels)\")\n","    print(f\"   Data type: {sample_frame.dtype}\")\n","    print(f\"   Value range: [{sample_frame.min()}, {sample_frame.max()}]\")"],"metadata":{"id":"u0amIrSVka_D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Exactly the same as Cell 3.2, but for deepfake videos instead of original videos\n","deepfake_frames = [] - Separate list for fake video frames\n","Everything else follows the same pattern"],"metadata":{"id":"YXIRNKKGkfc_"}},{"cell_type":"markdown","source":["## Cell 3.4: Summary of Frame Extraction"],"metadata":{"id":"UajY27N9kiO1"}},{"cell_type":"code","source":["total_frames = len(original_frames) + len(deepfake_frames)\n","\n","print(f\"‚úÖ Total frames extracted: {total_frames:,}\")\n","print(f\"   ‚Ä¢ Original (real): {len(original_frames):,} frames\")\n","print(f\"   ‚Ä¢ Deepfake (fake): {len(deepfake_frames):,} frames\")\n","\n","# Calculate balance\n","balance_ratio = len(original_frames) / len(deepfake_frames) if len(deepfake_frames) > 0 else 0\n","print(f\"\\n‚öñÔ∏è Dataset balance: {balance_ratio:.3f}:1 (real:fake)\")\n","\n","if 0.95 <= balance_ratio <= 1.05:\n","    print(\"   ‚úÖ Perfectly balanced!\")\n","elif 0.90 <= balance_ratio <= 1.10:\n","    print(\"   ‚úÖ Well balanced\")\n","else:\n","    print(\"   ‚ö†Ô∏è Slight imbalance detected\")\n","\n","# Memory estimate\n","bytes_per_frame = original_frames[0].nbytes if len(original_frames) > 0 else 0\n","total_memory_mb = (total_frames * bytes_per_frame) / (1024**2)\n","\n","print(f\"\\nüíæ Current memory usage:\")\n","print(f\"   ‚Ä¢ Bytes per frame: {bytes_per_frame:,} bytes\")\n","print(f\"   ‚Ä¢ Total raw frames: {total_memory_mb:.2f} MB\")\n","print(f\"   ‚Ä¢ After processing (64√ó64): ~{total_frames * 64 * 64 * 3 / (1024**2):.2f} MB\")\n","\n","print(f\"\\n‚úÖ Ready for image processing and tensor creation!\")"],"metadata":{"id":"TbdytdHqkcpF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["total_frames = len(original_frames) + len(deepfake_frames) - Counts all frames\n","balance_ratio = len(original_frames) / len(deepfake_frames) - Calculates balance\n","\n","Perfect balance = 1.0 (equal amounts)\n","\n","\n","1.0 = more real frames\n","\n","\n","< 1.0 = more fake frames\n","\n","\n","original_frames[0].nbytes - Gets memory size of one frame in bytes\n","\n",".nbytes is a numpy array property showing total bytes\n","\n","\n","(total_frames * bytes_per_frame) / (1024**2) - Converts to megabytes\n","\n","Divide by 1024 once = KB, twice = MB"],"metadata":{"id":"8ZAAa91ok1H6"}},{"cell_type":"markdown","source":["- Sample:"],"metadata":{"id":"h0YieHwDlF0g"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","print(\"üñºÔ∏è VISUALIZING FRAME AND NUMPY ARRAY\")\n","print(\"=\"*60)\n","\n","# Select a sample frame (let's take the first original frame)\n","sample_frame = original_frames[0]\n","\n","print(\"\\nüìä Numpy Array Properties:\")\n","print(f\"   Shape: {sample_frame.shape}\")\n","print(f\"   Data type: {sample_frame.dtype}\")\n","print(f\"   Min value: {sample_frame.min()}\")\n","print(f\"   Max value: {sample_frame.max()}\")\n","print(f\"   Memory size: {sample_frame.nbytes:,} bytes ({sample_frame.nbytes / (1024**2):.2f} MB)\")\n","\n","# Break down the shape\n","height, width, channels = sample_frame.shape\n","print(f\"\\nüìê Dimensions:\")\n","print(f\"   Height: {height} pixels\")\n","print(f\"   Width: {width} pixels\")\n","print(f\"   Channels: {channels} (Red, Green, Blue)\")\n","print(f\"   Total pixels: {height * width:,}\")\n","\n","# Show a small slice of the array (top-left corner, 3x3 pixels)\n","print(f\"\\nüîç Sample 3√ó3 pixels from top-left corner:\")\n","print(f\"   (Each pixel has 3 values: [R, G, B])\")\n","print(f\"\\n{sample_frame[:3, :3, :]}\")\n","\n","# Explain what we're seeing\n","print(f\"\\nüí° Interpretation:\")\n","print(f\"   ‚Ä¢ Each number represents color intensity (0-255)\")\n","print(f\"   ‚Ä¢ 0 = darkest (black), 255 = brightest (white)\")\n","print(f\"   ‚Ä¢ Each pixel has [R, G, B] values\")\n","print(f\"   ‚Ä¢ Example: [255, 0, 0] = pure red\")\n","print(f\"   ‚Ä¢ Example: [0, 255, 0] = pure green\")\n","print(f\"   ‚Ä¢ Example: [255, 255, 255] = white\")\n","\n","# Show statistics for each channel\n","print(f\"\\nüìà Channel Statistics:\")\n","red_channel = sample_frame[:, :, 0]\n","green_channel = sample_frame[:, :, 1]\n","blue_channel = sample_frame[:, :, 2]\n","\n","print(f\"   Red channel   - Mean: {red_channel.mean():.2f}, Std: {red_channel.std():.2f}\")\n","print(f\"   Green channel - Mean: {green_channel.mean():.2f}, Std: {green_channel.std():.2f}\")\n","print(f\"   Blue channel  - Mean: {blue_channel.mean():.2f}, Std: {blue_channel.std():.2f}\")\n","\n","# Visualize the frame\n","fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n","\n","# 1. Full RGB image\n","axes[0, 0].imshow(sample_frame)\n","axes[0, 0].set_title('Original Frame (RGB)', fontsize=14, fontweight='bold')\n","axes[0, 0].axis('off')\n","\n","# 2. Red channel only\n","axes[0, 1].imshow(sample_frame[:, :, 0], cmap='Reds')\n","axes[0, 1].set_title('Red Channel', fontsize=14, fontweight='bold')\n","axes[0, 1].axis('off')\n","\n","# 3. Green channel only\n","axes[1, 0].imshow(sample_frame[:, :, 1], cmap='Greens')\n","axes[1, 0].set_title('Green Channel', fontsize=14, fontweight='bold')\n","axes[1, 0].axis('off')\n","\n","# 4. Blue channel only\n","axes[1, 1].imshow(sample_frame[:, :, 2], cmap='Blues')\n","axes[1, 1].set_title('Blue Channel', fontsize=14, fontweight='bold')\n","axes[1, 1].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Show a zoomed-in section with pixel values\n","print(\"\\nüîé Zoomed view: 10√ó10 pixels from center with values\")\n","center_y = height // 2\n","center_x = width // 2\n","\n","# Extract small region\n","small_region = sample_frame[center_y:center_y+10, center_x:center_x+10, :]\n","\n","# Create visualization\n","fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n","ax.imshow(small_region)\n","ax.set_title('10√ó10 Pixel Region (Zoomed In)', fontsize=14, fontweight='bold')\n","\n","# Annotate each pixel with its RGB values\n","for i in range(10):\n","    for j in range(10):\n","        r, g, b = small_region[i, j, :]\n","        # Show RGB values on the image\n","        text_color = 'white' if (r + g + b) / 3 < 128 else 'black'\n","        ax.text(j, i, f'{r}\\n{g}\\n{b}',\n","                ha='center', va='center',\n","                color=text_color, fontsize=6,\n","                fontweight='bold')\n","\n","ax.set_xticks(range(10))\n","ax.set_yticks(range(10))\n","ax.grid(True, color='yellow', linewidth=1)\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\n‚úÖ Frame visualization complete!\")\n","print(\"\\nüí≠ Key Takeaway:\")\n","print(\"   The numpy array is just a 3D grid of numbers representing\")\n","print(\"   color intensities at each pixel location!\")"],"metadata":{"id":"h00-e2mllFZX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["^top left ==> black pixel"],"metadata":{"id":"wNoncFS_oMB1"}},{"cell_type":"code","source":["# Let's look at the CENTER of the frame instead of corner\n","sample_frame = original_frames[0]\n","height, width, channels = sample_frame.shape\n","center_y = height // 2\n","center_x = width // 2\n","\n","print(\"üîç CENTER 3√ó3 pixels (where face likely is):\")\n","center_region = sample_frame[center_y:center_y+3, center_x:center_x+3, :]\n","print(center_region)\n","\n","print(f\"\\nüí° Center pixel RGB value: {sample_frame[center_y, center_x, :]}\")\n","print(f\"   This should NOT be [0, 0, 0] if face is present!\")\n","\n","# Count black pixels\n","black_pixels = np.sum(np.all(sample_frame == 0, axis=2))\n","total_pixels = height * width\n","percent_black = (black_pixels / total_pixels) * 100\n","\n","print(f\"\\nüìä Black pixel analysis:\")\n","print(f\"   Total black pixels: {black_pixels:,} / {total_pixels:,}\")\n","print(f\"   Percentage black: {percent_black:.2f}%\")\n","\n","if percent_black > 50:\n","    print(\"   ‚ö†Ô∏è This frame is mostly letterbox/padding\")\n","elif percent_black > 20:\n","    print(\"   ‚ö†Ô∏è Frame has significant letterboxing\")\n","else:\n","    print(\"   ‚úÖ Frame has minimal letterboxing\")"],"metadata":{"id":"8UjW-bXvoLNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"dgxPb1eDk10w"}},{"cell_type":"markdown","source":["# SECTION 4: Image Processing & Tensor Creation"],"metadata":{"id":"tmrli9n3oa2I"}},{"cell_type":"markdown","source":["We'll take all the extracted frames, resize them to 64√ó64, normalize pixel values to [0, 1], convert from numpy arrays to PyTorch tensors, and create our final dataset tensors with proper shape (N, 3, 64, 64)."],"metadata":{"id":"s7S1EkyRoh2P"}},{"cell_type":"markdown","source":["## Cell 4.1: Resize All Frames to 64√ó64"],"metadata":{"id":"EzE0--SvoiiH"}},{"cell_type":"code","source":["target_size = CONFIG['image_size']  # 64\n","print(f\"Target size: {target_size}√ó{target_size} pixels\")\n","\n","# Function to resize a single frame\n","def resize_frame(frame, size):\n","    \"\"\"\n","    Resize a frame to target size.\n","\n","    Args:\n","        frame (numpy.ndarray): Input frame (H, W, C)\n","        size (int): Target size (will be size √ó size)\n","\n","    Returns:\n","        numpy.ndarray: Resized frame\n","    \"\"\"\n","    # cv2.resize takes (width, height) - note the order!\n","    resized = cv2.resize(frame, (size, size), interpolation=cv2.INTER_AREA)\n","    return resized\n","\n","# Resize all original frames\n","print(f\"\\nüìê Resizing {len(original_frames)} original frames...\")\n","original_frames_resized = []\n","\n","for frame in tqdm(original_frames, desc=\"Resizing original frames\"):\n","    resized = resize_frame(frame, target_size)\n","    original_frames_resized.append(resized)\n","\n","print(f\"‚úÖ Original frames resized: {len(original_frames_resized)}\")\n","\n","# Resize all deepfake frames\n","print(f\"\\nüìê Resizing {len(deepfake_frames)} deepfake frames...\")\n","deepfake_frames_resized = []\n","\n","for frame in tqdm(deepfake_frames, desc=\"Resizing deepfake frames\"):\n","    resized = resize_frame(frame, target_size)\n","    deepfake_frames_resized.append(resized)\n","\n","print(f\"‚úÖ Deepfake frames resized: {len(deepfake_frames_resized)}\")\n","\n","# Verify the new size\n","if len(original_frames_resized) > 0:\n","    sample = original_frames_resized[0]\n","    print(f\"\\nüñºÔ∏è Resized frame properties:\")\n","    print(f\"   Shape: {sample.shape} (was {original_frames[0].shape})\")\n","    print(f\"   Size reduction: {original_frames[0].nbytes / sample.nbytes:.1f}x smaller\")\n","    print(f\"   Memory per frame: {sample.nbytes:,} bytes\")\n","\n","total_memory = (len(original_frames_resized) + len(deepfake_frames_resized)) * sample.nbytes / (1024**2)\n","print(f\"\\nüíæ Total memory for resized frames: {total_memory:.2f} MB\")"],"metadata":{"id":"zLVSkf7jku6_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["def resize_frame(frame, size): - Creates reusable function for resizing\n","cv2.resize(frame, (size, size), interpolation=cv2.INTER_AREA) - Resizes the image\n","\n","(size, size) = target dimensions (64, 64)\n","interpolation=cv2.INTER_AREA = method for resizing (best for downsampling)\n","Note: OpenCV takes (width, height), not (height, width)!\n","\n","\n","\n","Resizing original frames:\n","\n","original_frames_resized = [] - New list for resized frames\n","for frame in tqdm(original_frames, ...): - Loop through each frame with progress bar\n","resized = resize_frame(frame, target_size) - Resize to 64√ó64\n","original_frames_resized.append(resized) - Add to new list\n","\n","Same process for deepfake frames\n","Verification:\n","\n","sample.shape - Shows new shape should be (64, 64, 3)\n","original_frames[0].nbytes / sample.nbytes - Shows how much smaller (e.g., 400x)\n","Calculates total memory used"],"metadata":{"id":"ZBzig-SrosJH"}},{"cell_type":"markdown","source":["## Cell 4.2: Normalize Pixel Values to [0, 1]\n"],"metadata":{"id":"PRYjpB-npHBh"}},{"cell_type":"code","source":["original_frames_normalized = []\n","\n","for frame in tqdm(original_frames_resized, desc=\"Normalizing original frames\"):\n","    # Convert from uint8 [0, 255] to float32 [0, 1]\n","    normalized = frame.astype(np.float32) / 255.0\n","    original_frames_normalized.append(normalized)\n","\n","print(f\"‚úÖ Original frames normalized: {len(original_frames_normalized)}\")\n","\n","# Normalize all deepfake frames\n","print(f\"\\nüìä Normalizing {len(deepfake_frames_resized)} deepfake frames...\")\n","deepfake_frames_normalized = []\n","\n","for frame in tqdm(deepfake_frames_resized, desc=\"Normalizing deepfake frames\"):\n","    # Convert from uint8 [0, 255] to float32 [0, 1]\n","    normalized = frame.astype(np.float32) / 255.0\n","    deepfake_frames_normalized.append(normalized)\n","\n","print(f\"‚úÖ Deepfake frames normalized: {len(deepfake_frames_normalized)}\")\n","\n","# Verify normalization\n","if len(original_frames_normalized) > 0:\n","    sample = original_frames_normalized[0]\n","    print(f\"\\nüîç Normalized frame properties:\")\n","    print(f\"   Data type: {sample.dtype} (was uint8)\")\n","    print(f\"   Value range: [{sample.min():.6f}, {sample.max():.6f}] (was [0, 255])\")\n","    print(f\"   Mean value: {sample.mean():.6f}\")\n","    print(f\"   Shape: {sample.shape} (unchanged)\")\n","\n","# Memory comparison\n","original_memory = original_frames_resized[0].nbytes\n","normalized_memory = sample.nbytes\n","print(f\"\\nüíæ Memory comparison:\")\n","print(f\"   Before (uint8): {original_memory:,} bytes per frame\")\n","print(f\"   After (float32): {normalized_memory:,} bytes per frame\")\n","print(f\"   Increase: {normalized_memory / original_memory:.1f}x (expected: 4x)\")"],"metadata":{"id":"-nII1R-xpIM7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["frame.astype(np.float32) - Converts data type from uint8 to float32\n","\n","uint8: 1 byte per value, range [0, 255]\n","float32: 4 bytes per value, can hold decimals\n","\n","\n","/ 255.0 - Divides by 255 to scale to [0, 1] range\n","\n","Example: 0 ‚Üí 0.0, 127 ‚Üí 0.498, 255 ‚Üí 1.0\n","\n","\n","The loop does this for every frame\n","\n","Why float32 instead of float64?\n","\n","float32 uses 4 bytes (sufficient precision for images)\n","float64 uses 8 bytes (overkill, wastes memory)\n","\n","Verification:\n","\n","sample.min() and sample.max() - Should be between 0.0 and 1.0\n","Memory increases 4x because float32 is 4 bytes vs uint8's 1 byte"],"metadata":{"id":"NBLdjDYdpNqJ"}},{"cell_type":"markdown","source":["## Cell 4.3: Convert to PyTorch Tensors (Channel-First Format)"],"metadata":{"id":"7-K4GAk7px3r"}},{"cell_type":"code","source":["print(\"üìå Converting from (H, W, C) to (C, H, W) format...\")\n","print(\"   NumPy format: (Height, Width, Channels) - e.g., (64, 64, 3)\")\n","print(\"   PyTorch format: (Channels, Height, Width) - e.g., (3, 64, 64)\")\n","\n","# Function to convert numpy array to PyTorch tensor\n","def numpy_to_tensor(frame):\n","    \"\"\"\n","    Convert numpy frame from (H, W, C) to PyTorch tensor (C, H, W).\n","\n","    Args:\n","        frame (numpy.ndarray): Frame in shape (H, W, C)\n","\n","    Returns:\n","        torch.Tensor: Tensor in shape (C, H, W)\n","    \"\"\"\n","    # Transpose from (H, W, C) to (C, H, W)\n","    # np.transpose reorders the dimensions\n","    # (0, 1, 2) ‚Üí (2, 0, 1) means: dimension 2 becomes first, then 0, then 1\n","    frame_chw = np.transpose(frame, (2, 0, 1))\n","\n","    # Convert to PyTorch tensor\n","    tensor = torch.from_numpy(frame_chw)\n","\n","    return tensor\n","\n","# Convert all original frames\n","print(f\"\\nüîÑ Converting {len(original_frames_normalized)} original frames to tensors...\")\n","original_tensors = []\n","\n","for frame in tqdm(original_frames_normalized, desc=\"Converting original to tensors\"):\n","    tensor = numpy_to_tensor(frame)\n","    original_tensors.append(tensor)\n","\n","print(f\"‚úÖ Original tensors created: {len(original_tensors)}\")\n","\n","# Convert all deepfake frames\n","print(f\"\\nüîÑ Converting {len(deepfake_frames_normalized)} deepfake frames to tensors...\")\n","deepfake_tensors = []\n","\n","for frame in tqdm(deepfake_frames_normalized, desc=\"Converting deepfake to tensors\"):\n","    tensor = numpy_to_tensor(frame)\n","    deepfake_tensors.append(tensor)\n","\n","print(f\"‚úÖ Deepfake tensors created: {len(deepfake_tensors)}\")\n","\n","# Verify conversion\n","if len(original_tensors) > 0:\n","    sample = original_tensors[0]\n","    print(f\"\\nüîç Tensor properties:\")\n","    print(f\"   Shape: {sample.shape} (C, H, W)\")\n","    print(f\"   Data type: {sample.dtype}\")\n","    print(f\"   Device: {sample.device} (cpu)\")\n","    print(f\"   Value range: [{sample.min():.6f}, {sample.max():.6f}]\")\n","    print(f\"   Is contiguous: {sample.is_contiguous()}\")\n","\n","# Memory check\n","print(f\"\\nüíæ Memory per tensor: {sample.element_size() * sample.nelement():,} bytes\")"],"metadata":{"id":"Xauk_i6fp004"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The transpose operation:\n","\n","np.transpose(frame, (2, 0, 1)) - Rearranges dimensions\n","\n","Input: (64, 64, 3) where indices are (0, 1, 2)\n","Output: (3, 64, 64) by moving dimension 2 to position 0\n","Visual example:\n","\n","\n","\n","    Before: frame[height, width, channel]\n","    After:  frame[channel, height, width]\n","Why this matters:\n","\n","NumPy/OpenCV convention: (H, W, C) - \"height first\"\n","PyTorch convention: (C, H, W) - \"channels first\"\n","PyTorch CNNs expect (C, H, W) format!\n","\n","Conversion:\n","\n","torch.from_numpy(frame_chw) - Creates PyTorch tensor from numpy array\n","\n","Shares memory (efficient, no copying)\n","Preserves data type (float32)\n","\n","\n","\n","Verification:\n","\n","sample.shape - Should be (3, 64, 64)\n","sample.device - Shows 'cpu' (we're not using GPU yet)\n","sample.is_contiguous() - Should be True (memory layout is optimal)"],"metadata":{"id":"_kG8_mjuq4i-"}},{"cell_type":"markdown","source":["^Contiguous: False"],"metadata":{"id":"3VfddT9EriS9"}},{"cell_type":"markdown","source":["## Cell 4.3 - UPDATED (Add .contiguous())\n"],"metadata":{"id":"qMNUU9s-rUOI"}},{"cell_type":"markdown","source":["Since we're about to save these tensors to disk, let's add .contiguous() in Cell 4.4 after creating the final X tensor"],"metadata":{"id":"RUb7bshwrfqo"}},{"cell_type":"markdown","source":["## Cell 4.4: Stack All Tensors and Create Labels (UPDATED)"],"metadata":{"id":"ps0iw__8rtk0"}},{"cell_type":"code","source":["# Stack all original tensors into one large tensor\n","print(f\"\\nüìö Stacking {len(original_tensors)} original tensors...\")\n","X_original = torch.stack(original_tensors)\n","\n","print(f\"‚úÖ X_original shape: {X_original.shape}\")\n","print(f\"   Interpretation: ({X_original.shape[0]} images, {X_original.shape[1]} channels, {X_original.shape[2]}√ó{X_original.shape[3]} pixels)\")\n","\n","# Stack all deepfake tensors into one large tensor\n","print(f\"\\nüìö Stacking {len(deepfake_tensors)} deepfake tensors...\")\n","X_deepfake = torch.stack(deepfake_tensors)\n","\n","print(f\"‚úÖ X_deepfake shape: {X_deepfake.shape}\")\n","print(f\"   Interpretation: ({X_deepfake.shape[0]} images, {X_deepfake.shape[1]} channels, {X_deepfake.shape[2]}√ó{X_deepfake.shape[3]} pixels)\")\n","\n","# Combine both into final dataset tensor\n","print(f\"\\nüîó Combining original and deepfake tensors...\")\n","X = torch.cat([X_original, X_deepfake], dim=0)\n","\n","# Make tensor contiguous in memory (ADDED)\n","print(f\"üîß Making tensor contiguous in memory...\")\n","X = X.contiguous()\n","\n","print(f\"‚úÖ Final X tensor shape: {X.shape}\")\n","print(f\"   Total images: {X.shape[0]:,}\")\n","print(f\"   Is contiguous: {X.is_contiguous()}\")  # Should now be True\n","\n","# Create labels\n","print(f\"\\nüè∑Ô∏è Creating labels...\")\n","# Labels for original frames (0 = real)\n","y_original = torch.zeros(len(original_tensors), dtype=torch.long)\n","# Labels for deepfake frames (1 = fake)\n","y_deepfake = torch.ones(len(deepfake_tensors), dtype=torch.long)\n","\n","# Combine labels\n","y = torch.cat([y_original, y_deepfake], dim=0)\n","\n","print(f\"‚úÖ Labels tensor shape: {y.shape}\")\n","print(f\"   Data type: {y.dtype}\")\n","print(f\"   Unique labels: {torch.unique(y).tolist()}\")\n","print(f\"   Label counts:\")\n","print(f\"      - Real (0): {(y == 0).sum().item()}\")\n","print(f\"      - Fake (1): {(y == 1).sum().item()}\")\n","\n","# Final verification\n","print(f\"\\n‚úÖ DATASET CREATION COMPLETE!\")\n","print(f\"=\"*60)\n","print(f\"üìä Final Dataset:\")\n","print(f\"   X shape: {X.shape} (images)\")\n","print(f\"   y shape: {y.shape} (labels)\")\n","print(f\"   X dtype: {X.dtype}\")\n","print(f\"   y dtype: {y.dtype}\")\n","print(f\"   X value range: [{X.min():.6f}, {X.max():.6f}]\")\n","print(f\"   y value range: [{y.min()}, {y.max()}]\")\n","print(f\"   X is contiguous: {X.is_contiguous()}\")  # Verification\n","print(f\"   y is contiguous: {y.is_contiguous()}\")  # y should already be contiguous\n","\n","# Memory usage\n","X_memory_mb = (X.element_size() * X.nelement()) / (1024**2)\n","y_memory_mb = (y.element_size() * y.nelement()) / (1024**2)\n","total_memory_mb = X_memory_mb + y_memory_mb\n","\n","print(f\"\\nüíæ Memory Usage:\")\n","print(f\"   X tensor: {X_memory_mb:.2f} MB\")\n","print(f\"   y tensor: {y_memory_mb:.2f} MB\")\n","print(f\"   Total: {total_memory_mb:.2f} MB\")"],"metadata":{"id":"FppZ60ULqpPD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Stacking tensors:\n","\n","torch.stack(original_tensors) - Combines list of tensors into one tensor\n","\n","Input: List of 500 tensors, each (3, 64, 64)\n","Output: One tensor of shape (500, 3, 64, 64)\n","Adds a new dimension at the beginning (batch dimension)\n","\n","\n","\n","Concatenating:\n","\n","torch.cat([X_original, X_deepfake], dim=0) - Joins tensors along dimension 0\n","\n","X_original: (500, 3, 64, 64)\n","X_deepfake: (500, 3, 64, 64)\n","Result: (1000, 3, 64, 64)\n","dim=0 means \"stack along first dimension\"\n","\n","\n","\n","Creating labels:\n","\n","torch.zeros(len(original_tensors), dtype=torch.long) - Creates tensor of all 0s\n","\n","Shape: (500,)\n","Data type: long (integer type for labels)\n","All values are 0 (representing \"real\")\n","\n","\n","torch.ones(len(deepfake_tensors), dtype=torch.long) - Creates tensor of all 1s\n","\n","Shape: (500,)\n","All values are 1 (representing \"fake\")\n","\n","\n","torch.cat([y_original, y_deepfake], dim=0) - Combines labels into (1000,)\n","\n","Why torch.long for labels?\n","\n","PyTorch loss functions (like CrossEntropyLoss) expect integer labels\n","torch.long is 64-bit integer type\n","\n","Memory calculation:\n","\n","X.element_size() - Bytes per element (4 for float32)\n","X.nelement() - Total number of elements (1000 √ó 3 √ó 64 √ó 64)\n","Multiply and convert to MB"],"metadata":{"id":"Tr28TcVzsF1D"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"6dhHoQxNtPaJ"}},{"cell_type":"markdown","source":["# SECTION 5: Dataset Statistics & Verification"],"metadata":{"id":"xuzaztJHtQPr"}},{"cell_type":"markdown","source":["We'll compute dataset statistics (per-channel mean and std), verify our tensors are correct, visualize sample images to ensure quality, and prepare metadata"],"metadata":{"id":"Ij2mEcIvtS4m"}},{"cell_type":"markdown","source":["## Cell 5.1: Calculate Per-Channel Statistics"],"metadata":{"id":"SaKm-YFetUd3"}},{"cell_type":"code","source":["# Calculate per-channel mean and standard deviation\n","print(\"\\nüßÆ Computing per-channel statistics across all images...\")\n","print(\"   This may take a moment for large datasets...\")\n","\n","# X shape is (N, C, H, W) = (1000, 3, 64, 64)\n","# We want to calculate mean/std across N, H, W dimensions\n","# Keep only the C (channel) dimension\n","\n","# Calculate mean for each channel\n","# dim=(0, 2, 3) means: average over batch, height, width\n","# Result: (3,) tensor with [R_mean, G_mean, B_mean]\n","mean = X.mean(dim=(0, 2, 3))\n","\n","# Calculate standard deviation for each channel\n","std = X.std(dim=(0, 2, 3))\n","\n","print(f\"\\n‚úÖ Statistics calculated!\")\n","print(f\"\\nüìà Per-Channel Statistics:\")\n","print(f\"   Red channel:\")\n","print(f\"      Mean: {mean[0]:.6f}\")\n","print(f\"      Std:  {std[0]:.6f}\")\n","print(f\"\\n   Green channel:\")\n","print(f\"      Mean: {mean[1]:.6f}\")\n","print(f\"      Std:  {std[1]:.6f}\")\n","print(f\"\\n   Blue channel:\")\n","print(f\"      Mean: {mean[2]:.6f}\")\n","print(f\"      Std:  {std[2]:.6f}\")\n","\n","# Overall statistics\n","print(f\"\\nüìä Overall Statistics:\")\n","print(f\"   Mean across all channels: {mean.mean():.6f}\")\n","print(f\"   Std across all channels: {std.mean():.6f}\")\n","\n","# Compare to common values\n","print(f\"\\nüí° Reference (ImageNet statistics):\")\n","print(f\"   Mean: [0.485, 0.456, 0.406]\")\n","print(f\"   Std:  [0.229, 0.224, 0.225]\")\n","print(f\"\\n   Your dataset may differ - that's normal!\")"],"metadata":{"id":"HUm7E6t_tgD1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mean calculation:\n","\n","X.mean(dim=(0, 2, 3)) - Calculates average across specific dimensions\n","\n","X shape: (1000, 3, 64, 64) = (batch, channels, height, width)\n","dim=(0, 2, 3) means: average over dimensions 0, 2, and 3\n","This averages over: all images (0), all heights (2), all widths (3)\n","Keeps: channel dimension (1)\n","Result: (3,) tensor = [red_mean, green_mean, blue_mean]\n","\n","Standard deviation:\n","\n","X.std(dim=(0, 2, 3)) - Same idea but for standard deviation\n","Measures spread/variation of pixel values\n","Higher std = more varied colors\n","Lower std = more uniform colors\n","\n","Why these stats matter:\n","\n","Used for standardization: X_standardized = (X - mean) / std\n","Helps neural networks train better (zero-centered data)\n","Your teammates can apply this if they want"],"metadata":{"id":"GxYir2QEtdB1"}},{"cell_type":"markdown","source":["## Cell 5.2: Verify Tensor Shapes and Properties\n"],"metadata":{"id":"r82TY1S8uZkH"}},{"cell_type":"code","source":["# Check X tensor\n","print(f\"\\nüìä X Tensor (Images):\")\n","print(f\"   Shape: {X.shape}\")\n","print(f\"   Expected: (N, 3, 64, 64) where N = number of images\")\n","\n","# Verify shape components\n","assert X.dim() == 4, f\"‚ùå X should have 4 dimensions, got {X.dim()}\"\n","assert X.shape[1] == 3, f\"‚ùå X should have 3 channels, got {X.shape[1]}\"\n","assert X.shape[2] == 64, f\"‚ùå X height should be 64, got {X.shape[2]}\"\n","assert X.shape[3] == 64, f\"‚ùå X width should be 64, got {X.shape[3]}\"\n","print(f\"   ‚úÖ Shape verification passed!\")\n","\n","# Check data type\n","print(f\"\\n   Data type: {X.dtype}\")\n","assert X.dtype == torch.float32, f\"‚ùå X should be float32, got {X.dtype}\"\n","print(f\"   ‚úÖ Data type verification passed!\")\n","\n","# Check value range\n","print(f\"\\n   Value range: [{X.min():.6f}, {X.max():.6f}]\")\n","assert X.min() >= 0.0, f\"‚ùå X minimum should be >= 0, got {X.min()}\"\n","assert X.max() <= 1.0, f\"‚ùå X maximum should be <= 1, got {X.max()}\"\n","print(f\"   ‚úÖ Value range verification passed!\")\n","\n","# Check for NaN or Inf\n","has_nan = torch.isnan(X).any().item()\n","has_inf = torch.isinf(X).any().item()\n","print(f\"\\n   Contains NaN: {has_nan}\")\n","print(f\"   Contains Inf: {has_inf}\")\n","assert not has_nan, \"‚ùå X contains NaN values!\"\n","assert not has_inf, \"‚ùå X contains Inf values!\"\n","print(f\"   ‚úÖ No NaN/Inf values detected!\")\n","\n","# Check if contiguous\n","print(f\"\\n   Is contiguous: {X.is_contiguous()}\")\n","assert X.is_contiguous(), \"‚ùå X should be contiguous!\"\n","print(f\"   ‚úÖ Memory layout verification passed!\")\n","\n","# Check y tensor\n","print(f\"\\nüìä y Tensor (Labels):\")\n","print(f\"   Shape: {y.shape}\")\n","print(f\"   Expected: (N,) where N matches X.shape[0]\")\n","\n","assert y.dim() == 1, f\"‚ùå y should have 1 dimension, got {y.dim()}\"\n","assert y.shape[0] == X.shape[0], f\"‚ùå y length should match X, got {y.shape[0]} vs {X.shape[0]}\"\n","print(f\"   ‚úÖ Shape verification passed!\")\n","\n","# Check data type\n","print(f\"\\n   Data type: {y.dtype}\")\n","assert y.dtype == torch.long or y.dtype == torch.int64, f\"‚ùå y should be long/int64, got {y.dtype}\"\n","print(f\"   ‚úÖ Data type verification passed!\")\n","\n","# Check label values\n","print(f\"\\n   Unique labels: {torch.unique(y).tolist()}\")\n","assert set(torch.unique(y).tolist()) == {0, 1}, f\"‚ùå Labels should be {{0, 1}}, got {torch.unique(y).tolist()}\"\n","print(f\"   ‚úÖ Label values verification passed!\")\n","\n","# Check balance\n","count_real = (y == 0).sum().item()\n","count_fake = (y == 1).sum().item()\n","balance_ratio = count_real / count_fake if count_fake > 0 else 0\n","print(f\"\\n   Real samples: {count_real}\")\n","print(f\"   Fake samples: {count_fake}\")\n","print(f\"   Balance ratio: {balance_ratio:.3f}\")\n","\n","if 0.9 <= balance_ratio <= 1.1:\n","    print(f\"   ‚úÖ Dataset is well balanced!\")\n","else:\n","    print(f\"   ‚ö†Ô∏è Dataset has some imbalance (still usable)\")\n","\n","print(f\"\\n\" + \"=\"*60)\n","print(f\"‚úÖ ALL VERIFICATIONS PASSED!\")\n","print(f\"=\"*60)"],"metadata":{"id":"k2jmRD0Lr97t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Assert statements:\n","\n","assert condition, \"error message\" - Checks if condition is True\n","\n","If True: continues silently\n","If False: raises error with message and stops execution\n","This is defensive programming - catches problems early!\n","\n","\n","\n","Shape verification:\n","\n","X.dim() == 4 - Checks tensor has 4 dimensions (batch, channels, height, width)\n","X.shape[1] == 3 - Checks 3 color channels (RGB)\n","X.shape[2] == 64 and X.shape[3] == 64 - Checks 64√ó64 size\n","\n","Value checks:\n","\n","torch.isnan(X).any() - Checks for NaN (Not a Number) values\n","\n","Returns True if ANY element is NaN\n",".item() converts single-value tensor to Python boolean\n","\n","\n","torch.isinf(X).any() - Checks for infinite values\n","Both should be False for valid data\n","\n","Why verify?\n","\n","Catches bugs early (wrong shape, wrong dtype, corrupted data)\n","Ensures data quality before saving"],"metadata":{"id":"omq30oRquidJ"}},{"cell_type":"markdown","source":["## Cell 5.3: Visualize Sample Images"],"metadata":{"id":"yiw8zBN6uo4v"}},{"cell_type":"code","source":["print(\"\\nüñºÔ∏è VISUALIZING SAMPLE IMAGES\")\n","print(\"=\"*60)\n","\n","import matplotlib.pyplot as plt\n","\n","# Select random samples to visualize\n","num_samples = 8  # 4 real + 4 fake\n","real_indices = torch.where(y == 0)[0][:4]  # First 4 real images\n","fake_indices = torch.where(y == 1)[0][:4]  # First 4 fake images\n","\n","# Create visualization\n","fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n","fig.suptitle('Sample Images from Dataset (After Preprocessing)', fontsize=16, fontweight='bold')\n","\n","# Plot real images (top row)\n","for i, idx in enumerate(real_indices):\n","    img = X[idx]  # Shape: (3, 64, 64)\n","\n","    # Convert from (C, H, W) to (H, W, C) for matplotlib\n","    img_hwc = img.permute(1, 2, 0)  # Now shape: (64, 64, 3)\n","\n","    # Display\n","    axes[0, i].imshow(img_hwc)\n","    axes[0, i].set_title(f'Real #{idx}', color='green', fontweight='bold')\n","    axes[0, i].axis('off')\n","\n","# Plot fake images (bottom row)\n","for i, idx in enumerate(fake_indices):\n","    img = X[idx]  # Shape: (3, 64, 64)\n","\n","    # Convert from (C, H, W) to (H, W, C) for matplotlib\n","    img_hwc = img.permute(1, 2, 0)  # Now shape: (64, 64, 3)\n","\n","    # Display\n","    axes[1, i].imshow(img_hwc)\n","    axes[1, i].set_title(f'Fake #{idx}', color='red', fontweight='bold')\n","    axes[1, i].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"‚úÖ Displayed 4 real and 4 fake sample images\")\n","print(f\"\\nüí° Image properties:\")\n","print(f\"   ‚Ä¢ Resolution: 64√ó64 pixels\")\n","print(f\"   ‚Ä¢ Normalized: Yes ([0, 1] range)\")\n","print(f\"   ‚Ä¢ Format: RGB (3 channels)\")"],"metadata":{"id":"Oanw34YruiKO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cell 5.4: Create Statistics Dictionary\n"],"metadata":{"id":"5r45_6cEu3z1"}},{"cell_type":"code","source":["# Create comprehensive statistics dictionary\n","dataset_stats = {\n","    # Dataset info\n","    'total_samples': X.shape[0],\n","    'num_real': (y == 0).sum().item(),\n","    'num_fake': (y == 1).sum().item(),\n","\n","    # Image properties\n","    'image_shape': list(X.shape[1:]),  # [3, 64, 64]\n","    'image_size': CONFIG['image_size'],\n","    'num_channels': X.shape[1],\n","\n","    # Value ranges\n","    'normalized': True,\n","    'value_range': [0.0, 1.0],\n","    'actual_min': X.min().item(),\n","    'actual_max': X.max().item(),\n","\n","    # Per-channel statistics\n","    'channel_mean': mean.tolist(),  # Convert tensor to list\n","    'channel_std': std.tolist(),\n","\n","    # Data types\n","    'X_dtype': str(X.dtype),\n","    'y_dtype': str(y.dtype),\n","\n","    # Labels\n","    'label_mapping': {\n","        'real': CONFIG['label_real'],\n","        'fake': CONFIG['label_fake']\n","    },\n","\n","    # Memory\n","    'X_memory_mb': (X.element_size() * X.nelement()) / (1024**2),\n","    'y_memory_mb': (y.element_size() * y.nelement()) / (1024**2),\n","\n","    # Quality checks\n","    'is_contiguous': X.is_contiguous(),\n","    'has_nan': torch.isnan(X).any().item(),\n","    'has_inf': torch.isinf(X).any().item(),\n","}\n","\n","# Print summary\n","print(\"\\nüìä Dataset Metadata Summary:\")\n","print(\"-\"*60)\n","for key, value in dataset_stats.items():\n","    if isinstance(value, float):\n","        print(f\"   {key}: {value:.6f}\")\n","    elif isinstance(value, list) and len(value) <= 5:\n","        print(f\"   {key}: {value}\")\n","    elif isinstance(value, dict):\n","        print(f\"   {key}:\")\n","        for k, v in value.items():\n","            print(f\"      {k}: {v}\")\n","    else:\n","        print(f\"   {key}: {value}\")"],"metadata":{"id":"GENa_IIwuhO0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"4T7WfuVQu8Bh"}},{"cell_type":"markdown","source":["# SECTION 6: Save, Compress & Download"],"metadata":{"id":"VyRxmYXVv_eT"}},{"cell_type":"markdown","source":["## Cell 6.1: Save Tensors to Disk"],"metadata":{"id":"b2KMTcUXwBvH"}},{"cell_type":"code","source":["# Define filenames\n","filename_X = \"FaceForensics_X.pt\"\n","filename_y = \"FaceForensics_y.pt\"\n","filename_stats = \"FaceForensics_stats.pt\"\n","\n","# Save X tensor (images)\n","print(f\"\\nüì¶ Saving X tensor to {filename_X}...\")\n","torch.save(X, filename_X)\n","file_size_X = os.path.getsize(filename_X) / (1024**2)\n","print(f\"‚úÖ Saved! Size: {file_size_X:.2f} MB\")\n","\n","# Save y tensor (labels)\n","print(f\"\\nüì¶ Saving y tensor to {filename_y}...\")\n","torch.save(y, filename_y)\n","file_size_y = os.path.getsize(filename_y) / (1024**2)\n","print(f\"‚úÖ Saved! Size: {file_size_y:.2f} MB\")\n","\n","# Save statistics\n","print(f\"\\nüì¶ Saving statistics to {filename_stats}...\")\n","stats_to_save = {\n","    'mean': mean,           # Per-channel mean\n","    'std': std,             # Per-channel std\n","    'metadata': dataset_stats  # All other metadata\n","}\n","torch.save(stats_to_save, filename_stats)\n","file_size_stats = os.path.getsize(filename_stats) / (1024**2)\n","print(f\"‚úÖ Saved! Size: {file_size_stats:.2f} MB\")\n","\n","# Summary\n","total_size = file_size_X + file_size_y + file_size_stats\n","print(f\"\\nüìä Save Summary:\")\n","print(f\"   {filename_X}: {file_size_X:.2f} MB\")\n","print(f\"   {filename_y}: {file_size_y:.2f} MB\")\n","print(f\"   {filename_stats}: {file_size_stats:.2f} MB\")\n","print(f\"   Total: {total_size:.2f} MB\")\n","\n","print(f\"\\n‚úÖ All tensors saved successfully!\")"],"metadata":{"id":"v5sBHpEiu6WO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["torch.save():\n","\n","torch.save(X, filename_X) - Saves PyTorch tensor to disk\n","\n","Serializes the tensor (converts to bytes)\n","Writes to a .pt file\n","Includes all metadata (shape, dtype, etc.)\n","Uses PyTorch's efficient binary format\n","\n","\n","\n","File size calculation:\n","\n","os.path.getsize(filename_X) - Gets file size in bytes\n","/ (1024**2) - Converts bytes to megabytes (MB)\n","\n","Stats dictionary:\n","\n","Combines mean, std, and metadata into one file\n","Your teammates load this one file to get all info"],"metadata":{"id":"4rgoH2lJwepH"}},{"cell_type":"markdown","source":["1. This is Colab's TEMPORARY storage\n","\n","‚ö†Ô∏è NOT in Google Drive (unless you mounted Drive and changed directory)\n","‚ö†Ô∏è Will be deleted when Colab session ends\n","‚ö†Ô∏è Temporary workspace that resets\n","\n","2. That's why we compress and download immediately\n","\n","Cell 6.3 creates the zip in /content/faceforensics_dataset.zip\n","Cell 6.4 downloads it to your PC\n","After download, you have it permanently on your computer"],"metadata":{"id":"MTvRTly7x0CQ"}},{"cell_type":"markdown","source":["## Cell 6.2: Verify Saved Files (Quality Check)\n"],"metadata":{"id":"5mJBb1D7wvkb"}},{"cell_type":"code","source":["print(\"\\nüìã Checking if files exist...\")\n","\n","# Check if all files were created\n","files_to_check = [filename_X, filename_y, filename_stats]\n","all_exist = True\n","\n","for filename in files_to_check:\n","    exists = os.path.exists(filename)\n","    size_mb = os.path.getsize(filename) / (1024**2) if exists else 0\n","    status = \"‚úÖ\" if exists else \"‚ùå\"\n","    print(f\"   {status} {filename}: {size_mb:.2f} MB\")\n","    all_exist = all_exist and exists\n","\n","if all_exist:\n","    print(f\"\\n‚úÖ All files exist!\")\n","else:\n","    print(f\"\\n‚ùå Some files are missing!\")\n","    raise FileNotFoundError(\"Not all files were saved correctly!\")\n","\n","# Load and verify one file to ensure it saved correctly\n","print(f\"\\nüî¨ Testing file integrity (loading X tensor back)...\")\n","X_loaded = torch.load(filename_X)\n","\n","# Verify it matches the original\n","print(f\"   Original shape: {X.shape}\")\n","print(f\"   Loaded shape: {X_loaded.shape}\")\n","assert X.shape == X_loaded.shape, \"‚ùå Shape mismatch!\"\n","\n","print(f\"   Original dtype: {X.dtype}\")\n","print(f\"   Loaded dtype: {X_loaded.dtype}\")\n","assert X.dtype == X_loaded.dtype, \"‚ùå Dtype mismatch!\"\n","\n","# Check if tensors are identical\n","tensors_match = torch.equal(X, X_loaded)\n","print(f\"   Tensors identical: {tensors_match}\")\n","assert tensors_match, \"‚ùå Tensors don't match!\"\n","\n","print(f\"\\n‚úÖ File integrity verified! Saved files are valid.\")\n","\n","# Clean up the loaded tensor to free memory\n","del X_loaded"],"metadata":{"id":"PVbHwKOmx-oU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["File existence check:\n","\n","os.path.exists(filename) - Returns True if file exists\n","Verifies all three files were actually created\n","\n","Integrity test:\n","\n","torch.load(filename_X) - Loads the saved tensor back\n","Reads the .pt file and reconstructs the tensor\n","Should be identical to original\n","\n","Verification:\n","\n","torch.equal(X, X_loaded) - Checks if tensors are exactly identical\n","\n","Compares every single element\n","Returns True only if perfectly identical\n","This proves no corruption during save\n","\n","\n","\n","Why this matters:\n","\n","Confirms the save operation worked correctly\n","Catches any disk errors or corruption\n","Gives you confidence before compressing"],"metadata":{"id":"UKRLDiN0yT5n"}},{"cell_type":"markdown","source":["## Cell 6.3: Compress Files into ZIP\n"],"metadata":{"id":"9FIwx55MyWMG"}},{"cell_type":"code","source":["import zipfile\n","import time\n","\n","# Define zip filename\n","zip_filename = \"faceforensics_dataset.zip\"\n","\n","print(f\"\\nüóúÔ∏è Creating {zip_filename}...\")\n","print(f\"   This may take a moment...\")\n","\n","start_time = time.time()\n","\n","# Create zip file\n","with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","    # Add each file to the zip\n","    for filename in files_to_check:\n","        print(f\"   üìÑ Adding {filename}...\")\n","        zipf.write(filename, arcname=filename)\n","\n","compression_time = time.time() - start_time\n","\n","print(f\"\\n‚úÖ Compression complete in {compression_time:.2f} seconds!\")\n","\n","# Check sizes\n","original_total_size = sum(os.path.getsize(f) for f in files_to_check) / (1024**2)\n","compressed_size = os.path.getsize(zip_filename) / (1024**2)\n","compression_ratio = (1 - compressed_size / original_total_size) * 100\n","\n","print(f\"\\nüìä Compression Summary:\")\n","print(f\"   Original size: {original_total_size:.2f} MB\")\n","print(f\"   Compressed size: {compressed_size:.2f} MB\")\n","print(f\"   Space saved: {compression_ratio:.1f}%\")\n","print(f\"   Compression ratio: {compressed_size / original_total_size:.2f}x\")\n","\n","print(f\"\\nüí° Note: Compression is LOSSLESS - no quality loss!\")"],"metadata":{"id":"k5iIwNixyJrb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["zipfile.ZipFile():\n","\n","zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) - Creates a zip file\n","\n","'w' = write mode (create new zip)\n","zipfile.ZIP_DEFLATED = use DEFLATE compression algorithm (standard ZIP compression)\n","Returns a file handle we can add files to\n","\n","\n","\n","Adding files:\n","\n","zipf.write(filename, arcname=filename) - Adds a file to the zip\n","\n","filename = path to the file on disk\n","arcname = name of the file inside the zip (same as original)\n","Compresses the file as it adds it\n","\n","\n","\n","Context manager (with):\n","\n","with zipfile.ZipFile(...) as zipf: - Automatically closes the zip when done\n","Ensures zip file is properly finalized\n","Even if errors occur, file is closed safely\n","\n","Compression statistics:\n","\n","Original size - sum of all three .pt files\n","Compressed size - size of the .zip file\n","Compression ratio = how much smaller (e.g., 0.85x = 15% saved)"],"metadata":{"id":"IPPm8A8Mypu4"}},{"cell_type":"markdown","source":["## Cell 6.4: Download the ZIP File\n"],"metadata":{"id":"E-Wx5WDvyvMl"}},{"cell_type":"code","source":["from google.colab import files\n","\n","print(f\"\\nüì• Initiating download of {zip_filename}...\")\n","print(f\"   Size: {compressed_size:.2f} MB\")\n","print(f\"   This will download to your browser's download folder\")\n","print(f\"\\n   ‚è≥ Please wait, do not close this tab...\")\n","\n","# Download the zip file\n","files.download(zip_filename)\n","\n","print(f\"\\n‚úÖ Download initiated!\")\n","print(f\"\\nüìù Next steps:\")\n","print(f\"   1. Check your browser's download folder\")\n","print(f\"   2. Locate: {zip_filename}\")\n","print(f\"   3. Extract the zip file\")\n","print(f\"   4. You'll find:\")\n","print(f\"      ‚Ä¢ your_dataset_X.pt (images)\")\n","print(f\"      ‚Ä¢ your_dataset_y.pt (labels)\")\n","print(f\"      ‚Ä¢ your_dataset_stats.pt (statistics)\")"],"metadata":{"id":"GwGsT_eTzJ7D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["files.download():\n","\n","files.download(zip_filename) - Triggers browser download\n","\n","Opens browser download dialog\n","Downloads file from Colab to your PC\n","Goes to your default downloads folder\n","\n","\n","\n","Important notes:\n","\n","This triggers immediately when cell runs\n","Browser will show download progress\n","Don't close the Colab tab until download completes!"],"metadata":{"id":"1_zxdfwwzSOE"}},{"cell_type":"markdown","source":["## Cell 6.7: Optional Cleanup\n"],"metadata":{"id":"LSl4LadI0AId"}},{"cell_type":"code","source":["print(\"Run this cell if you want to free up Colab disk space\")\n","print(\"(Only needed if running low on space)\")\n","\n","# Delete individual .pt files (we have the zip)\n","print(f\"\\nüóëÔ∏è Deleting individual .pt files...\")\n","for filename in files_to_check:\n","    if os.path.exists(filename):\n","        os.remove(filename)\n","        print(f\"   Deleted: {filename}\")\n","\n","# Delete extracted frames\n","print(f\"\\nüóëÔ∏è Clearing extracted frames from memory...\")\n","del original_frames\n","del deepfake_frames\n","del original_frames_resized\n","del deepfake_frames_resized\n","del original_frames_normalized\n","del deepfake_frames_normalized\n","del original_tensors\n","del deepfake_tensors\n","del X_original\n","del X_deepfake\n","\n","print(f\"\\n‚úÖ Cleanup complete!\")\n","print(f\"üíæ Freed up ~{total_size * 2:.2f} MB of space\")"],"metadata":{"id":"JXrbpsmw0BYu"},"execution_count":null,"outputs":[]}]}